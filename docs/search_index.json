[
["index.html", "My post-graduate study work Section 1 Introduction 1.1 About me", " My post-graduate study work Karine Villeneuve 2020-11-25 Section 1 Introduction Welcome to my bookdown, a place where I keep step-by-step guides and general informations about my project. 1.1 About me My name is Karine Villeneuve and I am a post-graduate student working at the lazar microbial ecology lab at the Univeristy of Quebec in Montreal. My research focuses on microbial communities that live in aquifers. More specifically I am interested in changes happening at the community level according to the seasons and in the migration of microorganism in the subsurface, from recharge to discharge area. I began my research in September 2019 after obtaining my bachelor’s degree in environmental studies at the University of Sherbrooke. This led me on an unpexpected journey from Montreal to Texas. "],
["host-bookdown-on-github.html", "Section 2 Host Bookdown on Github 2.1 Getting Bookdown started 2.2 Github Repository 2.3 From Bookdown to Github 2.4 Knit and upload to github", " Section 2 Host Bookdown on Github Before anything, I highly suggest going through Bookdown: Authoring Books and Technical Documents with R Markdown in order to understand the basics to setting up your own bookdown. 2.1 Getting Bookdown started From the Get started page download the Zip file, then unzip it locally. Install the RStudio IDE (you need a version higher than 1.0.0) Install the R package bookdown: # stable version on CRAN install.packages(&#39;bookdown&#39;) # or development version on GitHub # devtools::install_github(&#39;rstudio/bookdown&#39;) You should now have a folder called bookdown-demo-master which contains exemple chapters, an index and files needed for the Bookdown. In the Bookdown-demo-master folder Create a folder called docs and leave it empty Look for a file called _bookdown.yml Open the file Add output_dir: \"docs\" Exemple : book_filename: &quot;YOUR BOOK NAME HERE&quot; delete_merged_file: true language: ui: chapter_name: output_dir: &quot;docs&quot; 2.2 Github Repository Create a repository online Create a repository on Github where you will host your Bookdown (exemple test_book) Copy the repository link generated by Github Clone the newly created repository on your computer Open a terminal window Navigate to the folder where you want to have your Bookdown To clone your repository use the command git clone Exemple : git clone https://github.com/karinevilleneuve/test_book.git In this exemple, the Github link would be replaced by the link you copied earlier. 2.3 From Bookdown to Github Copy the entire content of the Bookdown folder into the new folder create by cloning the repository. In this case this folder is called test_book Now you can start modifying the content of your book by opening each chapter. What is now very important is that, once you knit your R Markdown, the ouput HTML will be created into the folder docs (the one we created earlier). This is important because Github will use the files from the folder docs to generate the Github page. 2.4 Knit and upload to github To upload your Bookdown to your github Buil book Open the file with the extension .Rproj Click on Buil Book under the Build tab on the right If you navigate to the folder docs you should now see that it contains a HTML copy of every chapter Update your online repository Open a terminal window and navigate to the repository folder now containing all the bookdown file Write the following command in order git status git add -f * git commit git push Go to your online Github repository, all the files and folders from your local folder should be there Go to Settings Scroll down to the section Github Pages Click on button None and from the dropdown menu select Master Click the /(root) button and from the dropdown menu select /(docs) Click Save The page will refresh, and now if you scroll back down to the section Github Pages you will see the URL of your Book Exemple Voilà "],
["methods.html", "Section 3 Methods", " Section 3 Methods We describe our methods in this chapter. "],
["applications.html", "Section 4 Applications 4.1 Example one 4.2 Example two", " Section 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["final-words.html", "Section 5 Final Words", " Section 5 Final Words We have finished a nice book. "],
["references.html", "References", " References "],
["texas-protocol.html", "Section 6 Texas Protocol 6.1 Unzip the gz files 6.2 Interleaving 6.3 Sickle 6.4 Fastqc 6.5 Fastq to fasta with seqtk 6.6 Assembling 6.7 POST ASSEMBLY STATS 6.8 Lenght and GC 6.9 Keeping sequence above 2000kb 6.10 BINNING 6.11 Depth file 6.12 MetaBAT 6.13 BIN QUALITY 6.14 BIN CLEANING 6.15 Depth file 6.16 BIN TAXONOMY 6.17 PHYLOGENETIC TREE 6.18 iTOL 6.19 METABOLIC PATHWAY 6.20 Hydrogenase tree 6.21 Mebs 6.22 Metabolic", " Section 6 Texas Protocol 6.1 Unzip the gz files gunzip *.gz Your files should now end in .fastq 6.2 Interleaving Run the interleave_fastq.py script using python python /home/karine/fastq/interleave_fastq.py MF_R1 MF_R2 &gt; MF_combined.fastq python /directory/to/script/nameofscript.py file_R1 file_R2 &gt; name_of_combined_file.fastq View interleaved file grep @M MF_combined.fastq | head grep (search) (???) (symbol or letter) nameofdocument | (pipeline) head (show me just the first lines) Move all interleaved fastq files into a single folder 6.3 Sickle Githup About Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3’-end of reads and also determines when the quality is sufficiently high enough to trim the 5’-end of reads. For a single file sickle pe -c MF_combined.fastq -t sanger -m MF_combined_trimmed.fastq -s MF_singles.fastq sickle pe (paired end) -c (inputfile) -t sanger (from illumina) -m (outputfilename) -s (exclutedreadsfilename) Bash script for multiple files `#!/bin/bash for i in *.fastq do sickle pe -c $i -t sanger -m $i.trim.fastq -s $i.singles.fastq done` 6.4 Fastqc Githup This will generate HTML files that you need to transfer to your local computer in order to view them in a chrome web page Run fastqc on the output (trimmed) file and the non-trimmed file fastqc MF_combined_trimmmed.fastq fastqc MF_combined.fastq This generates two HTML file (one for the combined_trimmed and one for the combined) Transfer files to your local computer Open both files (chrome web page) and compare them (check the quality) Running with Bash for multiple files fastqc *.fastq 6.5 Fastq to fasta with seqtk Githup Gzip the combined_trimmed.fastq file gzip MF_combined_trimmmed.fastq Convert the file frome fastq to fasta seqtk seq -a MF_combined_trimmed.gz &gt; MF.fa 6.6 Assembling There exists multiple assembling programs… it can be hard to chose the best one. I suggest trying a few and seeing what gives you the best results (number of sequence and average length of sequences) 6.6.1 IDBA Github Generate a folder called assemblyand the file we are interested in is called contig.fa Bash script using VI document Create a VI document named runidba.shcontaining the following script. To save and quit (ESC+ :qw) `#!/bin/bash for i in *.fa do idba_ud -l $i -o $i.assembly --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 i--num_threads 20 done Make the runidba.sh to make it executable chmod +x runidba.sh Run the script with nohup and &amp; (won’t be affected by connectivity problem) nohup ./runidba.sh &amp; View the tasks running on the server by using jobsor htop freeallows you to see the available memory For a single file idba_ud -l MF.fa -o assembly --pre_correction --mink 65 --maxk 115 --step 10 --seed_kmer 55 --num_threads 20 idba_ud -l (imput fasta file) -o (output directory name) --pre_correction --mink (minimum kmers length) --maxk (maximum kmers length) --step --seed_kmer --num_threads 6.6.2 Megahit Github NCBI MEGAHIT is very fast and memory efficient if time and RAM are causing a bottleneck. Set memory to whatever your machine can handle. This command is set up for interleaved fastq. Inside the folder containing your fastq files create a VI doc. called megahit.sh with the following script and make it executable #!/bin/bash for i in *.fastq do megahit --12 $i --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 25 -o /home/karine/megahit/$i --out-prefix Megahit_$i --memory 80441980 done The FASTA file Megahit_$1.contigs.fa is the output For a single file #!/bin/bash megahit --12 MF_combined_trimmed_fastq --k-list 21,33,55,77,99,121 --min-count 2 --verbose -t 25 -o /home/karine/fastq/megahit --out-prefix Megahit_MF --memory 80441980 By default, the cutoff value is 2, so k-mers occurring at least twice are kept while singleton k-mers are discarded. Because this eliminates not only sequencing errors, but also removes information from genuinely low abundant genome fragments. 6.6.3 metaSPAdes metaspades --12 MF_combined_trimmed_fastq -o /home/karine/fastq/metaspades -m 50 6.7 POST ASSEMBLY STATS Number of contigs grep -c &quot;&gt;&quot; contig.fa Lenght of contigs seqkit fx2tab --length --name --header-line contig.fa Histogram To obtain a histogram of the lenght of your contigs we first need to extract the column lenght from the document contig.fa with the command cut -f 4 length.tab into a new document called lenghts cut -f 4 length.tab &gt; lengths Remove the first row of the document lenghts (open the file in a text editor app like nano or vi) Use pipeline to create histogram with the lentghs documents less lenghts | Rscript -e &#39;data=abs(scan(file=&quot;stdin&quot;)); png(&quot;seq.png&quot;); hist(data,xlab=&quot;sequences&quot;, xlim=c(0,20000))&#39; The output is a png file called “seq.png” If x axis of the histogram is not right change the xlim=c(x,x) values Transfer the file to your local computer to view it and confirm how many sequences are greater than 2000 6.8 Lenght and GC High GC organisms tend not to assemble well and may have an uneven read coverage distribution. Run the perl script length+GC.pl /home/karine/fastq/assembly/length+GC.pl contig.fa &gt; contig_GC.txt View the GC content less contig_GC.txt 6.9 Keeping sequence above 2000kb perl -lne &#39;if(/^(&gt;.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print &quot;$s\\n$fa{$s}\\n&quot; if(length($fa{$s})&gt;2000) }}&#39; contig.fa &gt; 2000kb.fa Count how many you have above 2000 kb grep -c &quot;&gt;&quot; 2000kb.fa 6.10 BINNING Coverage-based binning approaches will require you to map reads to assembled contigs 6.10.1 Mapping BWA Reference Manual and Wrapper Github This wrapper maps FASTQ reads against an assembly (e.g. genome) in FASTA format using BWA-MEM. Put the fasta file with the long sequence as well as the trimmed-combined fastq file of each sample in the same folder Download the Github repository git clone https://github.com/imrambo/genome_mapping.git Create a conda environment under your home directory conda env create -f environment_linux64.yml Activate the conda environement conda activate scons_map Run a dry run (test run) scons --dry-run --fastq_dir=/home/karine/VP/megahit/concat --assembly=/home/karine/VP/megahit/concat/concatenated_1kb.fa --outdir=/home/karine/VP/megahit/concat/map --sampleids=fastq_concat.fastq --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/karine/tmp --logfile=concat.log Run the script scons --fastq_dir=/home/karine/VP/SV10 --assembly=/home/karine/VP/SV10/SV10_1kb.fa --outdir=/home/karine/VP/SV10_map --sampleids=SV10_combined --align_thread=5 --samsort_thread=5 --samsort_mem=768M --nheader=8 --tmpdir=/home/karine/tmp --logfile=SV10log By hand Index fasta file using bwa bwa index 2000kb.fa Align the fasta file against the combined_trimmed.fastqfile Create a vifile named mapping.shwith the following script #!/bin/bash bwa mem -t 30 2000kb.fa -p MF_combined_trimmed_fastq &gt; 2KB.MF.sam Make mapping.sh script executable chmod +x mapping.sh and run it script using nohup ./ and &amp; Convert .sam file to .bam file with samtools Create a vi file named samtools.sh with the following script #!/bin/bash samtools view -b -S 2KB.MF.sam &gt; 2KB.MF.bam -Sinput is a .sam file -boutput is a .bamfile Make script executable chmod +x samtools.sh and run the script in the background using nohup ./ and &amp; Sort the bam file (required in order to use the script to generate a depth file) create a vi file named sort.sh with the following script #!/bin/bash samtools sort -o 2KB.MF.sorted.bam 2KB.MF.bam Make script executable chmod +x sort.sh and run script using nohup ./ and &amp; 6.11 Depth file The depth allows you to know how many sequence you can align with certain sections of your contigs. Section with very little depth (few sequences) are not reputable to use ./jgi_summarize_bam_contig_depths --outputDepth depth.txt --pairedContigs paired.txt 2KB.MF.sorted.bam Open ouput depth file with less depth.txt 6.12 MetaBAT MetaBAT Efficient tool for accurately reconstructing single genomes from complex microbial communities Create vi file named run_metabat.sh with the following command #!/bin/bash metabat2 -i 2000kb.fa -a depth.txt -o bins_dir/bin -t 20 --minCVSum 0 --saveCls -d -v --minCV 0.1 -m 2000 minCVsum : assigning number of tetranucleotide frequency graphs, don’t grab negative numbers -m : min size of contig to be considered for binning Run Metabat using nohup ./and &amp; The output is a folder called bins_dir containing all the bins created 6.13 BIN QUALITY Github You have to go back one folder in the terminal (not be in the bins_dir folder) you may need to specify the extension of your file for it to work. For example, for file finishing is .fa the command will be checkm lineage_wf -x fa… checkm lineage_wf bins_dir/ bins_dir/checkm -f bins_dir/output.txt Open the output.txt document with excel to verify the completeness and contamination of your bins Standard : Completeness &gt; 50 % and Contamination &lt; 10 % Remove all the spaces with control + H Filter the columns by Completeness, and seperate the ones &lt; 50 % by adding a line in excel Filter by Contamination, and highlight all the ones &gt; 10 % - These are the bins you want to clean 6.14 BIN CLEANING MM Genome uses sequences from two different samples and binning is done by plotting the two coverage estimates against each other. Vizbin maps sequences based on tetranucleotide frequency. One can then manually create bins and check the quality of them using CheckM. If you have only one sample (one fastq file) Vizbin should be used. MM genome uses an R script that will require you to create a virutal environment (VE) 6.14.1 MM Genome Create a directory called genome_mapping under /home/username/ Clone the Github repository in the genome mapping directory git clone https://github.com/imrambo/genome_mapping.git Install miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Restart your terminal window See the SCons and local options while in genome_maping with scons -h Create a folfer named bins_cleaning In this folder make a nano text and paste the bin id of the bins you need to clean Add .fa at the end of each bin id with the folling command sed &#39;s/$/.fa/g&#39; bin_list.txt Check to see if everything is in order. If yes use -i to apply the changes sed -i &#39;s/$/.fa/g&#39; bin_list.txt Go to bins_dir and use this command to copy the bins that need cleaning in the folder bin_cleaning for i in `cat bin_list.txt`; do cp $i ../bin_cleaning/ ; done The Fastq files need to be gzip and in an other directory alone (fastq_compressed). The compression will take a lot of time so run it in screen The fastq_compressed is created in the Fastq directory. gzip -c MF_combined_trimmed_fastq &gt; fastq_compressed/MF_combined_trimmed_fastq.gz 6.14.2 Vizbin In your computer, create a folder call Vizbin and in this folder create a folder for each bins that need cleaning and import .fa of that bins from the server to your computer using FileZilla 6.15 Depth file In this example we are manually cleaning bin.9.fa a. Copy the bin (bin.9.fa) that you want to clean to a new folder called test_bin_cleaningin the fastq directory (you need to be in the bins_directory) cp bin.9.fa /home/karine/fastq/test_bin_cleaning/ b. Index the fasta file using bwa bwa index bin.9.fa c. Align the fasta file against the combined_trimmed.fastq file Create a vi file named mapping.sh with the folling script #!/bin/bash bwa mem -t 30 bin.9.fa -p /home/karine/fastq/MF_combined_trimmed_fastq &gt; bin.9.sam Run the script in the background using nohup ./ and &amp; nohup ./mapping.sh &amp; d. Convert .sam file to .bam file with samtools Use a vi script samtools view -b -S bin.9.sam &gt; bin.9.bam -Sinput is a .sam file -boutput is a .bamfile e. Sort the bam file Required in order to use the script to generate a depth file samtools sort -o bin.9.sorted.bam bin.9.bam f. Create the depth file You will need the jgi_summarize_bam_contig_depths (was located in midgard - SCP it to your folder ./jgi_summarize_bam_contig_depths --outputDepth depth.txt --pairedContigs paired.txt bin.9.sorted.bam To open the depth file less depth.txt 6.16 BIN TAXONOMY 6.16.1 Barrnap Copy all your cleaned bins into a new foler called cleaned_bins cp *.fa cleaned_bins/ We want to add the name of the bin to the beginning of every file and change the file type to .fna. Use this perl script : for i in *.fa ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$ARGV $1&quot;} else{ print }&#39; $i &gt; $i.fna ; done In each of your file, change the space between the name and the scaffhold numer to an underscore sed -i &#39;s/ /_/g&#39; *.fna Move all the .fnato a new directory (mkir fna) Concatenate all the .fna files in one document with the following command cat *.fna &gt; all_bins.fna Use barrnap to identify the scaffholds that have partial or complet 16S gene barrnap all_bins.fna &gt; barrnap_hits.txt barrnap --kingdom arc --lencutoff 0.2 --reject 0.3 --evalue 1e-05 all_bins.fna &gt; barrnap_archaea.txt barrnap --kingdom bac --lencutoff 0.2 --reject 0.3 --evalue 1e-05 all_bins.fna &gt; barrnap_bacteria.txt Opening the txt file will show you which scaffholds in which contigs have complete or near complete 16S rRNA. In our case, the bin.40 bin.40_1.fa_contig-115_1168 had the 16S. Therefor, we manually copied the sequence and pasted it in Blast ti identify the organism to which this sequence belonged. To obtain more sequences, we ran barrnap on the inital 2000kb.fa file. We obtained 11 scaffholds containing 16S. We copied the name of those scaffhold followed by the coordinates of the 16S gene in a vi file named scaff. Example : contig-115_1168:1-955 The following command was then used to copy the sequence identified in the vi scaff file into a new file name 16S_2KB.fna for i in `cat scaff`; do samtools faidx 2000kb.fa $i &gt;&gt;16S_2Kb.fna; done The extracted sequence where then uploaded to Blast and Silva website to identify the bacteria to which those sequence belong. 6.16.2 GTDBTK Github In the folder with all your clean and completed genomes run this command with nohup #!/bin/bash gtdbtk classify_wf --cpus 20 --genome_dir /home/karine/Bins/all_clean --out_dir gtdbk_output Once it is done running, you can open the folder called gtdbk_output and copy the folder gtdbtk.bac120.summary.tsv to your local computer in order to open it with excel. Use this folder to identify the phylum, class, order, family and genus that you need to download in order to construct your tree. 6.16.3 OneCodeX 6.17 PHYLOGENETIC TREE Download reference genomes Download assembly summary genbank file wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/assembly_summary_genbank.txt For each phylum, order, class, family of interest, search in the NCBI database and select 10 individuals with the most complete genome. Copy and paste the name and the assembly number in an excel document. From that excel document, coopy the list of assembly in a vi document called ref.txt Search for those identifiers in the assembly summary genbank file with the following command ; for n in $`cat ref.txt`; do grep $n assembly_summary_genbank.txt | cut -f 1,7,8,20 &gt;&gt; genomestodownload.tab ; done Modify the paths to download only the fna genomes cut -f 4 genomestodownload.tab | sed &#39;s/$/\\/*fna.gz/g&#39; &gt;&gt; ftp.links.txt Download the genomes and unzip the downloaded files wget -i ftp.links.txt gunzip *.gz Add the file name next to the orginial header for i in *.fna ; do perl -lne &#39;if(/^&gt;(\\S+)/){ print &quot;&gt;$1 [$ARGV]&quot;} else{ print }&#39; $i &gt; $i.MF_Ref.fna; done Keep only the files that end with _genomic.fna -&gt; you can move all the others to another folder Create a new directory and move your files there mkdir /home/karine/MF_ref mv *MF_Ref.fna /home/karine/MF_ref Run CheckM checkm lineage_wf -x fna /home/karine/MF_ref checkm -f output_table.txt Discard the genomes that are not completed. 6.17.1 Phylosift Githup Midgard Create a folder containing your completed .fna bins and reference genomes Create a vi file named run_phylosift.sh with the following command and run it using nohup ./ and &amp; #!/bin/bash find . -maxdepth 1 -name &quot;*.fna&quot; -exec /home/baker/phylosift_v1.0.1/bin/phylosift search --isolate --besthit {} \\; Once phylosift is done align the marker genes with the following command using nohup ./ and &amp; find . -maxdepth 1 -name &quot;*.fna&quot; -exec /home/baker/phylosift_v1.0.1/bin/phylosift align --isolate --besthit {} \\; Rename and concatenate the aligned marker for both bins find . -type f -regex &#39;.*alignDir/concat.updated.1.fasta&#39; -exec cat {} \\; | sed -r &#39;s/\\.1\\..*//&#39; &gt; name_of_bin_alignmnet.fasta 6.17.2 Geneious Download Geneious software (free 14 days trial) select new foler drag and drop the aligned fasta file Select Align/Assemble ; Multiple align MUSCLE Alignment … ; ok Once it is done, remove the bins or sequence that seem too small and re-run Multiple align - MUSCLE Alignment Once it is done, select the aligned file go to Tools ; Mask Align Export the file : File ; Export ; Selected Documents ; Phylip alignment (*phy) Move the file from your local computer to the server 6.17.3 RAxML Github In the folder with your .phy file create a vi file named tree.sh with the following command and run it using chmod +x ; nohup ./and &amp; #!/bin/bash raxmlHPC-PTHREADS-AVX -T 35 -f a -m PROTGAMMAAUTO -N autoMRE -p 12345 -x 12345 -s MF_aligned.phy -n MF_karine When the tree is done running, copy the file RAxML_bipartitionsBranchLabels.yourfilename to your local computer` 6.18 iTOL Create an iTOL account and upload your tree (RAxML_bipartitionsBranchLabels.yourfilename) and visualize the results Change the name of all the reference genomes Locate the folder where all the reference genoms are located and using terminal view the content of that foler using ls -l Copy/paste the name into an new tab of the excel document where you have saved the GCA assembly name of the reference genomes. Use VLOOKUP to match the GCA assembly name of the reference genomes to the organism name In another tab, copy the (1) column with the orginal name of the iTOL tree and the (2) column containing the matching Organism name (in this order very important) Copy these two column in a vi file named ref_name_tree in the same folder with your tree In the same folder create an executable perl script named replace_name.sh to change the name in your tree use strict; use warnings; my $treeFile = pop; my %taxonomy = map { /(\\S+)\\s+(.+)/; $1 =&gt; $2 } &lt;&gt;; push @ARGV, $treeFile; while ( my $line = &lt;&gt; ) { $line =~ s/\\b$_\\b/$taxonomy{$_}/g for keys %taxonomy; print $line; } Run the perl script perl replace_name.sh ref_name_tree RAxML_bipartitionsBranchLabels.MF_karine &gt; new_tree_name Upload the new created tree to iTOL 6.19 METABOLIC PATHWAY Convert bins to protein (.faa) In the folder with your bins in the .fna format… Make sure there is no space between the name of you bins and the scaffhold for i in *.fna ; do sed -i &#39;s/ /_/g&#39; $i ; done Run prodigal to convert your bins to amino acid sequence for i in *.fna ; do prodigal -i $i -o output.txt -a $i.faa ; done Move all the files ending with .faa to another folder mkdir faa ; mv *.faa /home/karine/Bins/all_clean/fna/faa Remove all the characters after the first space in the header for i in *.faa; do sed -i &#39;s/\\s.*$//&#39; $i; done &amp; Tranfer the files to your local computer Submit the files to kofamKOALA and HydDB - you can only submit one file at a time KofamKoala Submit one file at a time Accept the email Save each file as a text file identified with the name of the bin on your computer. HydDB Submit one file at a time and do not close the window while it runs. When it’s completed, download the excel and with the option text to column seperate the name of the name of bin/contigs and the class prediction. Use custom sort to sort by column B (the Hygrogenase group) copy only the groups that are hydrogenase ([FeFe], [NiFe], [fe]-) Paste these results in an excel document, combining all results from all bins 6.20 Hydrogenase tree In the server (takes a lot of memory) concatenate all your .faa files cat *.faa &gt; concat_faa.faa Create a vi document call hydDB_contigs.txt and copy the name of the sequence from your the excel document containing the name of the contigs identified as hydrogenase Extract those sequence from the concatenated file (count after using grep -c “&gt;” to make sure the numbers match) Option 1 - La plus rapide pullseq -i concat_faa.faa -n hydDB_contigs.txt&gt;&gt; hydDB_sequence.faa View the lenght of the extracted sequences seqkit fx2tab --length --name --header-line hydDB_sequence.faa &gt;&gt; hydDB_sequence.faa.fasta.lenght Remove all the smaller sequences (&lt;140) perl -lne &#39;if(/^(&gt;.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (keys(%fa)){ print &quot;$s\\n$fa{$s}\\n&quot; if(length($fa{$s})&gt;140) }}&#39; hydDB_sequence.faa &gt; long_sequence.faa Count how many you have grep -c &quot;&gt;&quot; long_sequence.faa Download the HydrogenaseDataBase fasta file located on the Google Drive Use diamond to compared your sequences with the onces in the DataBase diamond makedb --in HydroDB.fasta -d hydroDB Modify your .faa files to remove any space and turn them into 1 liner for i in *.faa; do sed -i &#39;s/\\s.*$//&#39; $i; done &amp; for i in *.faa ; do perl -lne &#39;if(/^(&gt;.*)/){ $head=$1 } else { $fa{$head} .= $_ } END{ foreach $s (sort(keys(%fa))){ print &quot;$s\\n$fa{$s}\\n&quot; }}&#39; $i &gt;&gt; $i.1L.faa ; done In the folder with all your .faa bins for i in *.faa; do diamond blastp --db hydroDB.dmd --query $i --out $i.hyd.tab --threads 3 --outfmt 6 qtitle stitle pident length qstart qend sstart send evalue bitscore --max-target-seqs 1 ; done Concat all the .hyd.tab files and open the file with excel. Add these as headers for your document qtitle | stitle | pident | length | Qstart | qend | Sstart | send | evalue | Bitscore | Use VLOOKUP to identify the hydrogenase sequence that were identified throught diamond blast and hydDB Keep only the sequence with an alignment length cutoff &gt; 40 amino acid residues and sequence identity &gt; 50% and Remove Group 4 Hydrogenase hit with sequence identity &lt; 60% Create a vi document call good_sequences.txt and extract (pullseq) those sequence from the file with all your sequence pullseq -i all_sequence.faa -n good_sequence.txt&gt;&gt; hydDB_diamond_filtered_sequences.faa Concat those sequences with the original GoogleDrive data base (I need to open the text document and search for bin, in order to do a manuel enter at the end of the data base sequence and the begining of my sequence, otherwise muscle won’t work because there will be &gt; in a sequence) Muscle align your sequences in the server (takes a lot of time on the computer) muscle -in long_sequence.faa -out aligned_long_sequence.faa -log log.txt -maxiters 2 Transfer files to geneious MAFFT and Mask align Eport the file as a Fasta to run fasttree Run fasttree command using nohup ./ and &amp; `#!/bin/bash fasttree hyd_align_comp.fasta &gt; hyd.tree Hydrogrenase Database sequence lenght To know the lenght of the sequence in the DataBase seqkit fx2tab --length --name --header-line hydroDB.faa &gt;&gt; HydroDB_length.faa Create a histogram cut -f 4 hydroDB_length &gt; DBlengths Open DBlengths with vi to remove the first row and run this command to generate the histogram less DBlengths | Rscript -e &#39;data=abs(scan(file=&quot;stdin&quot;)); png(&quot;seq.png&quot;); hist(data,xlab=&quot;lengths&quot;, xlim=c(0,1000)) &#39; 6.21 Mebs github git clone https://github.com/valdeanda/mebs.git Download mebs in the folder containing all your .faa files Mebs runs on python3 -&gt; download python3 on your computer (used brew install python) Once python3 is installed create a folder called Mebs containing all your .faa files Use pip install to download these four libraries apt-get install python3 python3-pip python3-matplotlib \\ ipython3-notebook python3-mpltoolkits.basemap pip3 install -U pip pip3 install --upgrade pandas $ sudo -H pip3 install --upgrade numpy pip3 install --upgrade scipy pip3 install --upgrade seaborn $ sudo -H pip3 install -U scikit-learn Run mebs using perl perl mebs.pl -input /home/karine/tree/faa -type genomic -comp &gt; MF_karine.tsv In the folder with the output file.tsv run the mebs python script python3 mebs_vis.py Phylo_5.tsv 6.22 Metabolic Githib Metabolic was run from the Student folder, with the following command within the folder with the FAA files perl /home/students2020/Tools/METABOLIC/METABOLiC-G.pl -in-gn /home/students2020/karine/folderwithFAA -o output/ -m /home/students2020/Tools/METABOLIC "]
]
